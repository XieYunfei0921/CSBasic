#### **数据仓库**

---

#### 决策支持系统

数据库应用可以广义的划分为**事务处理**和**决策支持系统**。

**事务处理系统**时用来记录事务有关的信息系统。

**决策支持系统**目标是从事务处理系统存储的细节信息中提取出高层次的信息，并利用这些高层次的信息进行各种决策。

使用决策支持的数据存储和检索会引入几个问题

+ 尽管许多决策支持查询可以使用SQL书写，但是还有一些无法通过sql表示，或者无法使用SQL简单表示，这是否提供几种SQL扩展，以便方便的进行数据分析。例如使用OLAP技术进行SQL扩展。
+ 数据库查询语言不适合对数据执行详细的统计分析
+ 大型公司拥有多样的数据源用于制定商务决策，这些数据源可能按照不同模式存储数据。基于性能考虑，不允许其他部门检索数据。为了能够高效查询,一些公司创建了数据仓库.数据仓库唯一单独的节点上,使用统一的模式收集多个数据源数据.
+ 知识发现技术实体从数据中发现统计规则和模式.数据挖掘领域和知识发现技术结构,使得能够使用与超大型数据库.

#### 数据仓库

1.  **定义**

   数据仓库时一个从多个数据源中收集信息,以统一的模式存储在单个站点的仓库(归档).一旦收集完毕,就会存储很长时间,运行访问历史数据.因此,数据仓库可以给用户提供单独的,统一的数据接口,易于支持决策支持查询的书写.

2. **数据仓库的成分**

   <img src="E:\截图文件\数据仓库架构.png" style="zoom:67%;" />

   + **何时和何地收集数据**

     在收集数据的源驱动架构中，数据源连续地或者周期性的传输新的信息。在目标驱动架构中，数据仓库周期性的将数据源发送需要的新数据请求。

   + **使用何种模式**

     单独构造的各个数据源可能具有不同模式，甚至是可以使用不同的数据模型。

   + **数据转换和清理**

     对数据的纠正和预处理任务称作数据清理，数据源经常传送大量具有略微不一致的数据，这种不一致性可以纠正。

     > 从多个数据源收集的地址列表可能含有重复，需要**合并清除操作**中去重

     数据除了清理，还需要采用多种方式进行**转换**。

   + **如何传播更新**

     数据源关系中的更新必须传递到数据仓库中，如果数据仓库中的关系与数据源一致则直接传递，否则就需要涉及视图维护的问题。

   + **汇总何种数据**

     事务处理系统产生的原始数据非常大，无法在线存储。然而，通过只维护关系上的聚集得到的汇总数据，而不是委会整个关系，可以回答许多查询。

3. **数据仓库模式**

   典型的数据仓库具有为设计分析设计的模式，例如OLAP模式。因此，数据通常是多个维度的，具有**维属性**和**度量属性**。包含多个维度数据的表称作**事实表**。

   为了最小化存储需求,维属性通常是一个短的标识,作为参照,其他的称作**维表**的表的外码.

   具备一个事实表,多个维表,以及事实表到维表外码的模式称作**星型模式**

   <img src="E:\截图文件\星型模式示例.png" style="zoom:67%;" />

   更复杂的数据仓库设计包含多个多级的维表.例如,iter_info具有manufacture_id属性,它是参照出厂商细节信息的另一个表的外码.这种模式称作**雪花模式**.

   复杂的数据仓库设计可能不止有一个**事实表**.

4. **面向列的存储**

   传统数据库把所有的元组存储在一起,在将元组存储在文件中,这种存储方式称作**面向行的存储**.相反,**面向列的存储**,关系中的每个列存储在单独的文件中.元组中相邻位置的元素的文件位置也相邻.假设属于定长记录类型,那么就可以通过上一个属性的文件位置计算出当前属性所在文件位置.

   > 面向列的存储优势:
   >
   > 1.  当查询需要包含大量属性关系的几个属性的时候,其余属性不需要从外存读取到内存的时候.行式存储会把不需要的属性字段放进内存中.
   > 2.  把相同类型的数据存储在一起提高了压缩效率,降低磁盘检索成本.
   >
   > 缺陷:
   >
   > 存储或者读取单个元组的时候需要多次IO.

#### 数据挖掘-决策树

1. **数据挖掘的定义**

   半自动地分析大型数据块以发行有用模式的处理过程,类似于人工智能的知识发现(机器学习).数据挖掘试图从数据中发现规则和模式.与机器学习不同的是,主要处理存储在磁盘上的大量数据.

   > 数据块中发现的某个类型知识可以使用一个**规则**表示,但是这样的规则总不是一定正确的,需要支持度和置信度.

2. **聚类**

   所发现的知识有大量的应用,最广泛的应用就是需要某种形式的**预测**.另一类就是寻找**关联**(相似度推荐),关联是**描述性模式**的一个例子,**聚类**是这种模式的另一个例子.

3. **分类**

   + **定义**

     给出某几个类之一的项,并给出向过去的实例(**训练实例**),以及它们所属的类,问题是预测一个新项所属的类.因为新实例的类是位置的,所以必须使用实例的其他属性来预测它所属的类.

     创建分量器的过程开始于数据样本,称作**训练集**.对训练集中每个元组,所属的类是已知的.

4. **决策树分类器**

   + **定义**

     决策树分类器使用一棵树,每个叶子节点有一个相关联的类,每个内部结点有一个与其相关的**谓词(函数)**。

     <img src="E:\截图文件\决策树示例.png" style="zoom:67%;" />

   + **构建决策树分类器**

     决策树的构建基于贪心算法，从根开始递归构建，起始时只有根节点和一个节点，所有训练的实例都应当与其相关联。

     每个节点上，几乎所有与该节点关联的训练实例都属于一个类，则该节点称为该类相关联的叶子节点。否则，必须选择一个**划分属性**和**划分条件**用于产生子节点。

     **与每个子节点相关联的数据**是满足该子节点划分条件的训练实例集合。

   + **最优划分**

     直观地,通过选择一个划分属性序列,从"不纯的"所有训练实例集合开始,到得到"纯的"叶子节点为止.在此"不纯的"表示包含了许多类的实例,"纯的"指的是每个叶子节点上所有训练实例同属于一个类.

     如何去定量的度量纯度,为了判断一个节点上选择特点属性和条件来划分数据的优势.测量利用该属性进行划分所得到的子节点上数据的纯度.选出得到的最大纯度的属性和条件.

     下面提供测定纯度的方法:

     假设有k个类,S的实例中属于类i的实例,占比为pi.

     > 有一种**基尼度量**的纯度测量方法:
     >
     > 定义:
     >
     > Gini(S)=1- sigma(1,k,pi)
     >
     > 当所有实例属于一个类的时候，Gini(S)=0.
     >
     > 在每个类有相同的实例数目情况下，它达到最大值`(1-1/k)`

     >另一种测量纯度的方法叫做**熵度量**,定义为:
     >
     >Entropy(S)= - sigma(1,k,pi*log pi)
     >
     >当所有实例属于单个类的时候,这个值为0.当每个类具有相同实例数目的情况下达到最大值.

     > 如果一个集合S可以划分为多个集合Si,可以按照下述方法测定该集合的结果集纯度:
     >
     > Purity(S1,S2...Sr)= sigma(1,r,abs(Si)/abs(S)*purity(S))
     >
     > 即，纯度为所有集合Si纯度的加权平均值。其中获得的**信息增益**为：
     >
     > Information_gain(S,{s1,s2...sr})=purity(S) - purity(S1,S2...Sr)
     >
     > 
     >
     > 划分为少数集合比划分成多个集合更加可取，前者可以产生更简单更有意义的决策树。每个集合Si中的元素数量也可能要考虑。否则，虽然划分对于所有元素一样，但是Si是否有0个或者1和元素，将在所分的集合数量上产生很大的差异。一个特定划分的**信息内容**可以使用熵的形式定义成：
     >
     > Information_content(S,{s1,s2...sr}) = - sigma(abs(Si)/abs(S) * log (abs(Si)/abs(S)))
     >
     > 对于某个属性的**最优划分**是能够给出**最大信息增益率**的划分:
     >
     > 定义为:
     >
     > Information_gain(S,{s1,s2...sr}) / Information_content(S,{s1,s2...sr}) 

   + **决策树构造算法**

     决策树的构造主要思路是计算**不同属性**和**不同的划分条件**.选择出能够产生最大信息增益率的属性和划分条件.

     同样的过程递归作用在每个划分结果集上,从而递归的构建一个决策树.

     > 如果数据可以很好的进行分类的情况下,递归过程在集合纯度为0的时候结束.然后通常情况下数据含有噪声,因此在纯度满足一定要求的时候就可以结束计算.

     将训练实例级S作为参数,当集合具有足够的纯度,或者S无法再进一步划分的时候,递归过程结束.参数deta(p)和deta(s)定义了纯度和集合大小的截止值.

     伪代码如下:

     ```markdown
     procedure GrowTree(S)
     	Partition(S)
     procedure Partition(S)
     	if(purity(S) > deta(p) || abs(S)<deta(s))
     		 return
     	对于每个属性A
     		计算基于属性A的划分
     		使用找到的最好划分，将S划分为S1，S2... Sr
     	for i: 1-> r
     		Partition(Si)
     ```

     对于非常大型的数据集,需要进行重复拷贝的工作,代价比较昂贵,因此算法在内存更大的情况下最小化IO和计算代价.

     一些算法对生成的决策树的子树进行修剪,以减小**过度适应**.如果一颗子树高度适应于特定的训练数据以至于其他数据产生分类错误,称作过度适应.

     子树的修剪将其替换成叶子节点来实现.其中含有两种修剪方式:

     > 1.  适应部分训练数据构造树,另一部分数据用于树的测试,当它发现如果一颗子树适应叶子节点代替会减少测试实例的分类错误的时候,就会修剪那棵树.

#### 数据挖掘- 其他分类器

+ 神经网络分类器
+ 贝叶斯分类器
+ 支持向量机